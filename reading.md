# What I'm reading

## 2025-10

- **Free Agents: How evolution gave us free will**, Kevin Mitchell, 2024<br> <br>
  *"We may say, 'I wish I were more confident like Gary,' [...] We rarely think, 'I wish I were more sociable like a naked mole rat,' [...] There seems to be little concern that we are constrained to behave like generic human beings, but strangely more worry that we are constrained to behave like ourselves."* In this book, Mitchell frames the question of free will as the extent to which our actions are guided by internal constraints rather than external ones. He identifies biological complexity as the core underpinning for our free will: it gives rise to a persistent self that can set future goals and act in accordance with them, as well as a sophisticated set of inhibitory mechanisms that serve as strong internal constraints on what we do. For him, it is precisely these internal constraints that give us free will, liberating us from control either entirely by external ones (as simpler organisms are) or unbridled randomness. The treatment of some philosophical nuances is lightweight, as is his treatment of some of the psychological literature, but he is true to his background as an evolutionary neuroscientist and brings an interesting and intuitively appealing perspective. Top-down organisation is a cause just as much as bottom-up mechanisms, and therein lies the agency. He leaves us with a final take on AI: agency is crucial, so artificial systems will never understand the world without causal interventions (which cannot be learned passively), goals, and indeterminacy.<br> <br>
    *Tags:* philosophy-of-mind

- **Slow Productivity: The lost art of accomplishment without burnout**, Cal Newport, 2024<br> <br>
  After popularising the term "deep work" in a previous, somewhat cult book, Newport turns his attention to what productivity really is (roughly: achieving goals), how to differentiate productivity from its antithesis pseudoproductivity (doing visible stuff to appear busy, regardless of whether it helps achieve those goals), and his three principles for "slow productivity". Slow productivity is the philosophy that hgh-quality work that's worth doing takes time, and not just determination and a long to-do list. He calls on examples of famous over-achievers from history to demonstrate the value of his proposed approach, and while this does resemble cherry-picking or survivorship bias, it works well as a pep talk. I have been reflecting lately on whether the general expectation that everything should be done unstoppably faster, in many areas of life and society, eventually hist a wall of human cognitive limits. Newport's three principles align with my concerns: he recommends doing fewer things, working at a natural pace, and obsessing over quality. Current trends promoted by productivity gurus and company management (exacerbated by the arrival of LLMs) suggest a very different idea of what productivity is and how to achieve it. Perhaps productivity-as-a-goal becomes an example of Goodhart's law.<br> <br>
  *Tags:* productivity

- **A hardwired neural circuit for temporal difference learning**, Campbell et al. (2025), bioRxiv ([link](https://www.biorxiv.org/content/10.1101/2025.09.18.677203v2))<br> <br>
  An impressive and important paper, finally confirming the widely-held theory that ventral striatum transmits value signals to the ventral tegmental area, which in turn relays a temporal-difference error signal back to the striatum. The authors do a series of minutely detailed experimental studies (in a total of 74 mice!) to identify the exact transformations that each cell type in the circuit performs, bridging the gap from computational algorithms which are common in this subfield to biological filters that implement them.<br> <br>
  *Tags:* systems-neuroscience, reinforcement-learning<br>

- **Empire of AI: Inside the reckless race for total domination**, Karen Hao, 2025<br> <br>
  A perfect complement to *Code Dependent* (see below), telling the other side of the story: a biography of OpenAI that illustrates how the oppression felt by people across the world arises from Silicon Valley. Ego, financial interests, ideological hubris and an attitude of empire-building - along with the huge capital available in Silicon Valley - enabled a small handful of people to develop large language models without the safety checks or ethical considerations that are usually expected of such new technologies. Like Murgia's message, Hao ends with a call for those affected by AI technologies to be included in its development from the start.<br> <br>
  *Tags:* ai-ethics<br>

## 2025-09

- **A neural manifold view of the brain**, Perich et al. (2025), Nature Neuroscience ([link](https://rdcu.be/ex8hW))<br> <br>
  An accessible and comprehensive overview of one of the topics that made me excited to study neuroscience (when I read Miguel Nicolelis's book *Beyond Boundaries*, long before my PhD). The authors make a convincing argument that neural manifolds solve the tricky problem of what to do with all the data we can now record thanks to recent developments in neurophysiological recording technologies: they offer a human-interpretable link between large amounts of low-level data and abstracted phenomena such as behaviour.<br> <br>
  *Tags:* computational-neuroscience<br>

## 2025-08

- **Code Dependent: Living in the Shadow of AI**, Madhumita Murgia, 2025<br> <br>
  A thoughful and balanced book telling human stories about the good and bad side of AI, from data annotators to under-resourced doctors to the parents of troubled teenagers: some using AI, some helping to develop it, some subjected to it, and some fighting back. A few of the tales are familiar ones that are often repeated in the increasing number of books about AI ethics, but Murgia's deliberate focus on collecting a global distribution of perspectives emphasises how, despite the potential for AI to improve people's lives across the world, it is often used as a tool to oppress by governments and enterprises alike.<br> <br>
  *Tags:* ai-ethics<br>
  

- **Beyond the Supply Chain: Artificial Intelligence's Demand Side**, Alicia Solow-Niederman, SSRN ([link](https://substack.com/redirect/921e88e8-992f-4061-bcbb-473884a8fbcc?j=eyJ1IjoiZTFpenIifQ.tj7HWegqye0jV0E0dW3m6Db6gT7WqPK7DVM9mUQAfgY))<br> <br>
  Found via Luiza Jarovsky's [AI governance newsletter](https://www.luizasnewsletter.com/).<br> <br>
  Solow-Niederman highlights some privacy issues that can arise via the (often innocent) interaction between a user and a generative AI system, which she argues are often overlooked. For example, a user asked a chatbot to generate some code, ChatGPT generated a URL, and the URL happened to lead to a real photo of a person's face on imgur. I recall (and now can't find) a social media post from some months ago in which people asked ChatGPT (with "memory" enabled, such that it had access to a large volume of chats from the same user) where in the world it thought they were from and what it thought their native language was, and users reported pretty high accuracy. Is the way you speak to AI a privacy violation by itself?<br> <br>
  *Tags:* ai-governance






